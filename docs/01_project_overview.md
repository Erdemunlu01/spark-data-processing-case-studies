# Project Overview

This project contains **three practical data processing and analysis case studies** implemented using Apache Spark.  
The studies focus on common real-world data challenges and demonstrate how these challenges can be handled using Spark in a clear and practical way.

All case studies in this project are based on **CSV datasets stored on HDFS**.

The main goal is to present an end-to-end Spark data processing workflow without getting lost in heavy theoretical details.

---

## Project Purpose

The primary objectives of this repository are:

- To gain hands-on experience with reading and processing data using Apache Spark
- To demonstrate how dirty datasets can be cleaned using Spark
- To work with real-world datasets using the Spark DataFrame API
- To perform simple and understandable data analyses based on business needs
- To document the completed work clearly and professionally on GitHub

---

## Scope

This project includes the following case studies:

### 1. Data Cleaning
- Processing a dirty retail dataset using Apache Spark
- Normalizing inconsistent text fields
- Converting numeric fields to appropriate data types

### 2. Real Data Processing
- Reading a real-world dataset from HDFS
- Applying basic Spark DataFrame transformations
- Producing datasets suitable for analysis

### 3. Business-Oriented Data Analysis
- Working with multiple CSV tables
- Performing joins and data transformations
- Generating analysis outputs aligned with business scenarios

---

## Technologies Used

The following technologies are used in this project:

- Apache Spark (PySpark)
- HDFS (Hadoop Distributed File System)
- Jupyter Notebook
- Linux command-line tools

---

## Working Principles

- The code inside the notebook files is **not modified**
- The documentation focuses on explaining what each step does and why it is required
- The project is designed to be learning- and practice-oriented
- Advanced optimizations or complex Spark features are intentionally not included

---

## Target Audience

This project is intended for:
- Beginners learning Apache Spark
- Individuals practicing data engineering or data analysis
- Anyone looking for practical examples of working with Spark on real datasets
