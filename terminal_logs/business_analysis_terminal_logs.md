# business_analysis_terminal_logs

This document describes the terminal commands used during the **business-oriented analysis**
performed on the **Retailer dataset**, and explains **why** each command was executed.

This file documents the **operational and validation steps** that support the analyses
performed inside the Spark notebooks.

---

## 1) Retailer Dataset Checks on HDFS

### Listing the main retailer directory
```bash
hdfs dfs -ls /user/train/retailer_db
```

**Explanation:**  
This command verifies that the main retailer dataset directory exists on HDFS
and that the expected subdirectories are correctly created.

---

### Checking sub-table directories
```bash
hdfs dfs -ls /user/train/retailer_db/categories
hdfs dfs -ls /user/train/retailer_db/customers
hdfs dfs -ls /user/train/retailer_db/departments
hdfs dfs -ls /user/train/retailer_db/orders
hdfs dfs -ls /user/train/retailer_db/order_items
hdfs dfs -ls /user/train/retailer_db/products
```

**Explanation:**  
These commands validate that the CSV files for each retailer table
are located in the correct HDFS directories.

---

## 2) Spark Analysis Output Validation

### Listing the analysis output directory
```bash
hdfs dfs -ls /user/train/retailer_output
```

**Explanation:**  
Checks whether the output directory generated by Spark
exists on HDFS after running business analysis jobs.

---

### Inspecting Spark output files
```bash
hdfs dfs -ls /user/train/retailer_output/part-*
```

**Explanation:**  
Lists the distributed `part-*` files created by Spark.
This confirms that the analysis results were successfully written.

---

## 3) Post-Processing Checks for Partitioning and Bucketing (If Applied)

### Verifying partitioned directory structure
```bash
hdfs dfs -ls /user/train/retailer_output/partition_column=*
```

**Explanation:**  
If the Spark output is written using partitioning,
this command verifies that partition directories were created correctly.

---

## 4) File Size and Data Volume Checks

```bash
hdfs dfs -du -h /user/train/retailer_output
```

**Explanation:**  
Displays disk usage information for the analysis output directory.
Useful for validating data volume and confirming successful writes.

---

## 5) Spark Job Monitoring via YARN

### Listing active YARN applications
```bash
yarn application -list
```

**Explanation:**  
Checks whether Spark analysis jobs are running on YARN
and displays their current status.

---

### Viewing Spark application logs
```bash
yarn logs -applicationId <application_id>
```

**Explanation:**  
Retrieves logs for a specific Spark application.
Used to inspect errors, warnings, or performance-related issues
during business analysis execution.

---

## Notes

- This file contains terminal commands specific to the **business analysis case study**
- Data joins and analytical logic are implemented inside the Spark notebooks
- These commands focus on **monitoring, validation, and operational control**
- The goal is to ensure the workflow is **traceable** and **reproducible**

---

These terminal logs complement the Spark-based retailer business analysis
by documenting its operational aspects.
