# data_cleaning_terminal_notes

This document summarizes the terminal commands used during the **Data Cleaning** case study
and explains **why** each command was executed.

> Note: This file documents operational steps such as HDFS directory/file checks and Spark output validation,
> which are performed outside the Spark notebooks and independently from the Python/Spark code.

---

## 1) HDFS Directory Preparation and Validation

### Checking the working directory
```bash
hdfs dfs -ls /user/train/spark_odev_transaction
```

**Explanation:**  
This command checks whether the target HDFS directory used in the data cleaning workflow exists
and lists its contents.

---

## 2) Uploading the Dataset to HDFS (Optional)

If the dataset is located in the local environment and needs to be uploaded to HDFS:

```bash
hdfs dfs -put dirty_store_transactions.csv /user/train/datasets
```

**Explanation:**  
Copies the local dataset file into the target HDFS directory.  
Since Spark reads data from HDFS, the dataset must be stored there.

---

## 3) Validating Spark Output

### Listing the output directory after Spark write
```bash
hdfs dfs -ls /user/train/spark_odev_transaction
```

**Explanation:**  
After running the Spark job, this command verifies that the output directory
and generated files exist on HDFS.  
Typically, `part-*` files and a `_SUCCESS` file are expected.

---

### Listing Spark output part files
```bash
hdfs dfs -ls /user/train/spark_odev_transaction/part-*
```

**Explanation:**  
Lists the distributed output files generated by Spark.
This confirms that the data was successfully written in parallel.

---

## 4) Checking Output Size

```bash
hdfs dfs -du -h /user/train/spark_odev_transaction
```

**Explanation:**  
Displays the disk usage of the output directory in a human-readable format.
Useful for validating that data was written and for estimating data volume.

---

## 5) Spark Job Monitoring via YARN (When Needed)

### Listing active YARN applications
```bash
yarn application -list
```

**Explanation:**  
Checks whether Spark jobs are currently running on YARN and shows their status.

---

### Viewing Spark application logs
```bash
yarn logs -applicationId <application_id>
```

**Explanation:**  
Retrieves logs for a specific Spark application.
Used for error analysis, warnings, and performance troubleshooting.

---

## Notes

- All commands listed here belong to the **data cleaning case study**.
- Data transformations (STORE_LOCATION normalization, `$` removal, type casting, etc.)
  are performed inside the Spark notebook; this file focuses on **HDFS and job validation steps**.
- The goal is to make the workflow **reproducible** and **traceable**.

---

These terminal logs complement the Spark-based data cleaning workflow
by documenting the operational and validation steps.
